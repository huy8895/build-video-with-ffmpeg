# split_text.py
"""
Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n â‰¤ max_char, báº£o toÃ n ranh giá»›i cÃ¢u.
Log ra tiáº¿n trÃ¬nh báº±ng icon Ä‘á»ƒ dá»… theo dÃµi.
YÃªu cáº§u: nltk (pip install nltk) â€“ chá»‰ táº£i 'punkt' (~1 MB) láº§n Ä‘áº§u.
"""

import re
from typing import List

import nltk
from nltk.tokenize import sent_tokenize

# Báº£o Ä‘áº£m tokenizer 'punkt' cÃ³ sáºµn (láº§n Ä‘áº§u CI táº£i ráº¥t nhanh)
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    print("ğŸ“¥  nltk: táº£i tokenizer 'punkt'â€¦")
    nltk.download("punkt", quiet=True)


def _normalize(text: str) -> str:
    """
    Chuáº©n hoÃ¡ vÄƒn báº£n nháº¹:  â€fooâ€â†’ â€. fooâ€, gá»™p khoáº£ng tráº¯ng thá»«a.
    """
    text = re.sub(r'â€\s+', 'â€ ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def split_text(text: str, max_char: int) -> List[str]:
    """
    Parameters
    ----------
    text : str
        VÄƒn báº£n nguá»“n.
    max_char : int
        Giá»›i háº¡n kÃ½ tá»± má»—i chunk.

    Returns
    -------
    List[str]
        Danh sÃ¡ch cÃ¡c Ä‘oáº¡n Ä‘Ã£ cáº¯t (dÆ°á»›i max_char).
    """
    print(f"ğŸ”  Äang chuáº©n hoÃ¡ & tÃ¡ch cÃ¢uâ€¦")
    text = _normalize(text)
    sentences = sent_tokenize(text)
    print(f"âœï¸   Tá»•ng sá»‘ cÃ¢u phÃ¡t hiá»‡n: {len(sentences)}")

    chunks, current = [], ""
    for idx, sentence in enumerate(sentences, 1):
        proposed = (current + " " + sentence).strip() if current else sentence
        if len(proposed) <= max_char:
            current = proposed
        else:
            if current:
                chunks.append(current)
                print(f"âœ‚ï¸   Táº¡o chunk #{len(chunks)} ({len(current)} kÃ½ tá»±)")
            current = sentence
    if current:
        chunks.append(current)
        print(f"âœ‚ï¸   Táº¡o chunk #{len(chunks)} ({len(current)} kÃ½ tá»±)")

    print(f"âœ… HoÃ n táº¥t chia: {len(chunks)} chunk (â‰¤{max_char} kÃ½ tá»±)\n")
    return chunks

